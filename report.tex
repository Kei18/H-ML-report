\documentclass[a4paper,10pt]{jsarticle}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}

\title{Machine Learning Midterm Assignment}
\author{Keisuke Okumura, 18M31013}

\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits}
\newcommand{\coloneq}{:=}

\begin{document}
\maketitle

\section*{Problem 1}
ロジスティック回帰では次の問題を解くことになる.
\[ \hat{\bm{w}} = \argmin_{\bm{w}}J(\bm{w}) \]
\[
 J(\bm{w}) \coloneq \sum_{i=1}^{n}(
 \ln{(1 + \exp{(-y_i\bm{w}^\mathrm{T}\bm{x}_i)}}))
 + \lambda \bm{w}^\mathrm{T}\bm{w}
\]

最急降下法(steepest gradient method)では, 以下の更新式に従って学習を行う. $\alpha$ は定数とする.
\[
\bm{w}^{(t+1)} = \bm{w}^{(t)} - \alpha\left.\frac{\partial J}{\partial \bm{w}}\right|_{\bm{w}=\bm{w}^{(t)}}
\]
\begin{align*}
\frac{\partial J(\bm{w})}{\partial \bm{w}}
 &= \frac{1}{n}\sum_{i=1}^{n}
 \frac{\exp(-y_i\bm{w}^\mathrm{T}\bm{x}_i)(-y_i\bm{x}_i)}
 {1 + \exp(-y_i\bm{w}^\mathrm{T}\bm{x}_i)}
 + 2\lambda\bm{w}\\
 &=\frac{1}{n}\sum_{i=1}^{n}
 \frac{-y_i\bm{x}_i}
 {1 + \exp(y_i\bm{w}^\mathrm{T}\bm{x}_i)}
 + 2\lambda\bm{w}\\
\end{align*}

また, ニュートン法(newton method)では以下の更新式に従って学習を行う. $\alpha$ は定数とする.
\[ \bm{w}^{(t+1)} = \bm{w}^{(t)} + \alpha\bm{d}^{(t)} \]
\[ \nabla^2J(\bm{w}^{(t)})\bm{d}^{(t)} = -\nabla J(\bm{w}^{(t)})
\Longleftrightarrow
\bm{d}^{(t)} = - (\nabla^2J(\bm{w}^{(t)}))^{-1} \nabla J(\bm{w}^{(t)})
\]

$\nabla^2J(\bm{w})$は次のように計算される.
\[
 \nabla^2J(\bm{w}) = \frac{1}{n}\sum_{i=1}^{n}(\frac{\exp(-y_i\bm{w}^\mathrm{T}\bm{x}_i)}
 {(1 + \exp(-y_i\bm{w}^\mathrm{T}\bm{x}_i))^2})\bm{x}_i\bm{x}_i^\mathrm{T}) + 2\lambda\bm{I}
\]

$100$個のデータからなるデータセットを Toy Dataset I\hspace{-1pt}I を参考に生成し,
最急降下法, 及びニュートン法を実行した結果を図.~\ref{img:log-result}に示す.
また, その時の学習の進行の様子を図.~\ref{img:log-loss}に示す.
パラメータ更新の繰り返し数は$50$回, $\alpha=0.1$, $\lambda=0.1$として学習を行った.

\begin{figure}[htbp]
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=7cm]{figs/p1_logistic-regression_result.pdf}
  \end{center}
  \caption{ロジスティック回帰の学習結果. $\bm{w}^\mathrm{T}\bm{x}=0$ を描画.}
  \label{img:log-result}
 \end{minipage}
 \begin{minipage}{0.5\hsize}
  \begin{center}
   \includegraphics[width=7cm]{figs/p1_logistic-regression_loss.pdf}
  \end{center}
  \caption{損失関数の値の変化}
  \label{img:log-loss}
 \end{minipage}
\end{figure}

\section*{Problem 2}
本問題では, \textit{lasso}を用いて次の問題を考える.
\[
 \hat{\bm{w}} = \argmin_{\bm{w}}((\bm{w}-\bm{\mu})^\mathrm{T}\bm{A}(\bm{w}-\bm{\mu})
 + \lambda||\bm{w}||_1)
\]
解を得るために, Proximal gradient methodを用いる.
$\psi = (\bm{w}-\bm{\mu})^\mathrm{T}\bm{A}(\bm{w}-\bm{\mu})$ とする.
$\psi$ を二次形式とみなし $\bm{A}$ を対称行列とすると, $\nabla \psi$ は次のようになる.
\[
 \nabla \psi = (\bm{A} + \bm{A}^\mathrm{T})(\bm{w} - \bm{\mu}) = 2\bm{A}(\bm{w} - \bm{\mu})
\]

したがって, 更新式は次のようになる.
\[
 w^{(t+1)}_i
 = ST_{\frac{\lambda}{\gamma}}\left(w^{(t)}_i
 -\left\{\frac{1}{\gamma}\nabla\psi(\bm{w}^{(t)})\right\}_i\right)
 = ST_{\frac{\lambda}{\gamma}}\left(w^{(t)}_i
 -\left\{\frac{1}{\gamma}2\bm{A}(\bm{w}^{(t)} - \bm{\mu})\right\}_i\right)
\]
\begin{align*}
 ST_q(\mu) = \begin{cases}
              \mu - q &(\mathit{if} \,\, \mu > q)\\
              0 &(\mathit{if} \,\, |\mu| \leq q)\\
              \mu + q &(\mathit{if} \,\, \mu < -q)\\
             \end{cases}
\end{align*}

以下の条件で, 学習を行う.
\[
 \bm{A} = \left(\begin{array}{cc} 3 & 0.5 \\ 0.5 & 1 \end{array}\right), \,
 \bm{\mu} = \left(\begin{array}{c} 1 \\ 2 \end{array}\right)
\]
この時 $\bm{A}$ の固有値は$\frac{4\pm\sqrt{5}}{2}$であるから,
学習率を$\eta_t=\left(\frac{4+\sqrt{5}}{2}\right)^{-1}=\frac{8-2\sqrt{5}}{11}$とする.

\end{document}